## 一、流任务阔缩容

### 1. 无状态任务

对于无状态的任务，更改并行度非常简单，它只需要在改完并行度以后重启任务，如下图所示

![image-20230606083842059](https://raw.githubusercontent.com/MingRongXi/my-study-picture/master/%E6%97%A0%E7%8A%B6%E6%80%81%E9%98%94%E7%BC%A9%E5%AE%B9.png)

### 2. 有状态任务

但是，对于有状态的任务在做阔缩容调整的时候就复杂多了，因为我们需要重新分发之前的状态。

#### 2.1 Flink状态的本地性

Flink为了保证其高吞吐、低延迟的性能，必须减少网络通信，在Flink中网络通信仅发生在沿着operator graph的边的地方（垂直），所有Flink的数据只能从上游发送到下游；但是，并行的算子实例之间并没有通信（水平），为了避免这种的通信，数据本地性是关键的原则，本地性也深刻影响着状态怎么存储和访问。

为了数据本地性，Flink的所有状态数据总是绑定到运行相应并行算子实例的任务，并且共存于运行该任务的同一台机器上。

明白了本地性以及禁止“水平”网络通信，我们就知道在阔缩容时直接把一个算子上的状态发送到另一个算子上是不可能的，如上图，在缩容时直接把2的状态发送到1上，这是不可能实现的。

#### 2.2 基于CheckPoint做阔缩容

前面讲了我们不能直接在算子之间进行通信发送状态，所以需要一个中间的存储，在Flink中这个中间存储就是CheckPoint。CheckPoint在做快照时，会把算子的状态存储到分布式存储系统上，如HDFS。

基于存储，我们在做阔缩容时，算子就可以访问存储系统来获得自己需要的状态数据了，如下图：

第一步，checkpoint被触发然后发送到dfs；

第二步，任务更改并行度后重启然后从dfs上获取。

![image-20230606090636480](https://raw.githubusercontent.com/MingRongXi/my-study-picture/master/%E5%BC%95%E5%85%A5dfs%E7%9A%84%E9%98%94%E7%BC%A9%E5%AE%B9.png)

此时，我们需要解决的问题是在更改到新并行度后如何对老并行度产生的状态做划分，下面介绍两种不合理的方式：

1. 把dfs中的状态向map_1、map_2、map_3都发送一份，这种就会导致过多的网络开销。
2. 直接把老的map_1和map_2的状态映射到新map_1和map_2上，此时就会导致3空闲出来，而且，还可能会导致数据问题，如果source 到 map之间是keyBy的操作，那之前是按照并行度2做的keyBy，现在按照3做，会导致相同的数据在keyBy之后与其状态不在同一个task上。

下面我们介绍一下Flink中用的阔缩容策略，Operate State和Keyed State有各自的策略

## 二、Operator State阔缩容

### 1. 引入示例

一个常见的operator state的使用场景是在kafka source中保存每个 partition的offset，每个kafka source维护一个<PartitionId, Offset>的键值对，这个键值对就会被保存到operator state中。那么在发生阔缩容的时候，我们怎么重新分配所有的键值对呢，理想情况下，是以轮询的方式重新划分。

<img src="https://raw.githubusercontent.com/MingRongXi/my-study-picture/master/operatorstate%E9%98%94%E7%BC%A9%E5%AE%B9.png" alt="image-20230606094733019"  />

上图显示了Operator State做checkPoint时的两种策略，第一种已弃用

### 2. 老策略（弃用）

如图Figure 2A,在做checkpoint时把state里的所有数据当作一个整体生成一个Object对象，在kafka source中，这个对象就是一个partition offset的list。

不管是在做checkpoint写还是读时，只能将其作为一个整体来操作。

这种方法在做阔缩容时存在问题：Flink如何将operator state分解为有意义、可重新分配的分区？尽管我们知道kafka source中存放的就是partition offset的list，但是这种方法返回的state对于flink来说是黑盒的，它无法将其重新分配。

一种可能可以实现扩容的方法是将整体状态分发到各个节点，然后operator之间互相交流读取哪一段，但是Flink在水平方向是不能通信的，所以这种方法也不能用。

### 3. 新策略

为了解决黑盒的问题，Flink对CheckPoint借口进行了调整，把Checkpointed改成了ListCheckpointed，如图Figure 2B，它返回和接受的都是一个list，list中的元素对于flink来说是黑盒，但是它被flink认为是独立的、可重新分配的。

## 三、Keyed State阔缩容

不同于Operator State，Keyed State是作用在每一个key上的，只能在KeyedStream中使用，即keyBy()之后使用。因为对于每一个key，只会有唯一的算子实例负责，所以KeyedState是和算子实例绑定的。我们需要解决的问题是如何从本地存储的checkpoint中恢复状态，如下图所示，怎么把之前3个subtask的状态映射到4个subtask上。

![image-20230612082316454](https://raw.githubusercontent.com/MingRongXi/my-study-picture/master/%E7%9B%B4%E6%8E%A5%E7%94%A8hash%20key%E7%9A%84%E9%98%94%E7%BC%A9%E5%AE%B9.png)

一个简单的想法就是每个新的subtask读取之前全部老subtask落下来的checkpoint，然后从中根据hask(key)过滤出自己需要的数据。但是这种会导致每个subtask都读取很多不相关的状态数据，而且dfs也会接收到很大的读请求。

既然直接读取数据文件不行，那我们可以为每个key建立索引，在索引文件中记录每个key在checkpoint文件中的位置，这样所有的子任务就都可以非常有选择性地定位和读取匹配的key，避免不相关数据的读取。但是这种方式存在两个缺点，i. 当key很多时，索引文件会变得非常大；ii. 在dfs中会产生非常大量的随机读（随即读会导致dfs的性能非常差）。

Flink采取的策略是介于这两种方法之间的，它引入了**key-group作为状态分配的最小单元**。每个key都会落在唯一的key-group中，一个key-group可以容纳多个key，在做阔缩容时，只需要调整每个subtask读入的key-group即可，如下图所示。

key-group的生成规则：key_group(key) = hash(key) % max_parallelism

subtask读入的key：subtask(key) = key_group(key) * parallelism / max_parallelism，因为key_group(key)的范围是[0, max_parallelism)，所以key_group(key) * parallelism / max_parallelism的范围就是[0, parallelism)。

如下图所示，在缩小并行度的时候，每个subtask读入的key_group个数就更多，增大并行度的时候，每个subtask读入的key_group个数就更少。

![image-20230612203014705](https://raw.githubusercontent.com/MingRongXi/my-study-picture/master/image-20230612203014705.png)
